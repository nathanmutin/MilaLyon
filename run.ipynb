{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da195a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from missing_codes import DEFAULT_MISSING, EXCEPTIONS\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'C:/Users/ACER/OneDrive - epfl.ch/Desktop/ML/dataset/'\n",
    "x_train, x_test, y_train, train_ids, test_ids, train_columns, test_columns = load_csv_data(data_folder, sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485642cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_clean = replace_missing(x_train, DEFAULT_MISSING, EXCEPTIONS)\n",
    "x_test_clean = replace_missing(x_test, DEFAULT_MISSING, EXCEPTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cf27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NaNs in x:\", np.isnan(x_test).sum())\n",
    "print(\"Number of NaNs in x_test:\", np.isnan(x_test_clean).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reduced, x_test_reduced, kept_cols = drop_too_many_missing(x_train_clean, x_test_clean, train_columns,threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb133e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_imputed, x_test_imputed = mean_imputation(x_train_reduced, x_test_reduced, np.array(train_columns)[kept_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm, x_test_norm = min_max_normalize(x_train_imputed, x_test_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation matrix (rows=features)\n",
    "corr = np.corrcoef(x_train_norm, rowvar=False)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04, label=\"Correlation\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optionally print highly correlated pairs\n",
    "threshold = 0.9\n",
    "print(f\"\\nHighly correlated features (|corr| > {threshold}):\")\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i + 1, corr.shape[1]):\n",
    "        if abs(corr[i, j]) > threshold:\n",
    "            print(f\"  {train_columns[i]} ↔ {train_columns[j]} : {corr[i, j]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final, x_test_final, kept_cols, dropped_features = drop_highly_correlated(\n",
    "    x_train_norm,\n",
    "    x_test_norm,\n",
    "    feature_names=train_columns,\n",
    "    threshold=0.9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39da43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clip_outliers(x_train, x_test=None, n_std=3):\n",
    "    \"\"\"\n",
    "    Clips outliers to within mean ± n_std * std for each feature.\n",
    "    Reports how many values were clipped.\n",
    "\n",
    "    Args:\n",
    "        x_train (np.array): shape=(N,D) training feature matrix\n",
    "        x_test (np.array, optional): shape=(M,D) test feature matrix\n",
    "        n_std (float): number of standard deviations for clipping\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            x_train_clipped (np.array): clipped training data\n",
    "            x_test_clipped (np.array or None): clipped test data (if provided)\n",
    "            n_clipped (int): number of values clipped in training set\n",
    "    \"\"\"\n",
    "    mean = np.mean(x_train, axis=0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    clip_min = mean - n_std * std\n",
    "    clip_max = mean + n_std * std\n",
    "\n",
    "    # Count values to be clipped (before applying np.clip)\n",
    "    below_min = x_train < clip_min\n",
    "    above_max = x_train > clip_max\n",
    "    n_clipped = np.sum(below_min | above_max)\n",
    "\n",
    "    # Apply clipping\n",
    "    x_train_clipped = np.clip(x_train, clip_min, clip_max)\n",
    "\n",
    "    print(f\"Clipped {n_clipped} values in x_train \"\n",
    "          f\"({n_clipped / x_train.size * 100:.2f}% of all entries)\")\n",
    "\n",
    "    if x_test is not None:\n",
    "        below_min_test = x_test < clip_min\n",
    "        above_max_test = x_test > clip_max\n",
    "        n_clipped_test = np.sum(below_min_test | above_max_test)\n",
    "        x_test_clipped = np.clip(x_test, clip_min, clip_max)\n",
    "        print(f\"Clipped {n_clipped_test} values in x_test \"\n",
    "              f\"({n_clipped_test / x_test.size * 100:.2f}%)\")\n",
    "        return x_train_clipped, x_test_clipped, n_clipped, n_clipped_test\n",
    "\n",
    "    return x_train_clipped, n_clipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e86c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce(x_train, x_test=None, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Perform PCA and reduce dimensionality to preserve given variance.\n",
    "\n",
    "    Args:\n",
    "        x_train (np.array): training data, shape (N, D)\n",
    "        x_test (np.array, optional): test data, shape (M, D)\n",
    "        variance_threshold (float): fraction of variance to keep (e.g. 0.95)\n",
    "\n",
    "    Returns:\n",
    "        x_train_pca (np.array)\n",
    "        x_test_pca (np.array or None)\n",
    "        eigvecs (np.array): principal component directions\n",
    "        explained_variance (np.array): explained variance ratio per component\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    # Covariance\n",
    "    cov = np.cov(x_train, rowvar=False)\n",
    "\n",
    "    # Eigen decomposition\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "\n",
    "    # Sort by descending eigenvalue\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals, eigvecs = eigvals[idx], eigvecs[:, idx]\n",
    "\n",
    "    # Compute explained variance\n",
    "    explained_variance = eigvals / np.sum(eigvals)\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "    # Determine number of components\n",
    "    k = np.searchsorted(cumulative_variance, variance_threshold) + 1\n",
    "    print(f\"Keeping {k} components explaining {cumulative_variance[k-1]*100:.2f}% variance\")\n",
    "\n",
    "    # Project data\n",
    "    x_train_pca = np.dot(x_test, eigvecs[:, :k])\n",
    "\n",
    "    if x_test is not None:\n",
    "        \n",
    "        x_test_pca = np.dot(x_test, eigvecs[:, :k])\n",
    "        return x_train_pca, x_test_pca, eigvecs[:, :k], explained_variance[:k]\n",
    "\n",
    "    return x_train_pca, eigvecs[:, :k], explained_variance[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca, x_test_pca, eigvecs, var_ratio = pca_reduce(\n",
    "    x_train_norm,\n",
    "    x_test_norm,\n",
    "    variance_threshold=0.95\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, len(var_ratio) + 1), var_ratio, 'o-', linewidth=2)\n",
    "plt.title(\"Explained Variance per Component\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d89881",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance = np.cumsum(var_ratio)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.arange(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', linewidth=2)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance threshold')\n",
    "plt.title(\"Cumulative Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2928322",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = (y_train == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000      # number of gradient descent steps\n",
    "gamma = 0.01          # learning rate\n",
    "initial_w = np.zeros(x_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = logistic_regression(y_train_bin, x_train_pca, initial_w, max_iters, gamma)\n",
    "print(f\"Final training loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec41b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(tx, w, threshold=0.5):\n",
    "    pred = sigmoid(tx @ w)\n",
    "    return (pred >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = predict_labels(x_train_pca, w)\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "acc_train = compute_accuracy(y_train_bin, y_pred_train)\n",
    "\n",
    "print(f\"Training accuracy: {acc_train*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(os.path.join(data_folder, \"x_train.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b7cc5",
   "metadata": {},
   "source": [
    "## Test of the algorithms on a linear regression between 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138237d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observe that features 1 and 2 are correlated\n",
    "# Let's test our algorithms on these two features only\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from implementations import mean_squared_error_gd, mean_squared_error_sgd, least_squares\n",
    "\n",
    "tx = np.ones((x_train.shape[0], 2))\n",
    "tx[:, 1] = x_train[:, 1]\n",
    "\n",
    "w_gd, _ = mean_squared_error_gd(x_train[:,2], tx, np.array([0., 0.]), max_iters=100, gamma=0.1)\n",
    "w_sgd, _ = mean_squared_error_sgd(x_train[:,2], tx, np.array([0., 0.]), max_iters=10000, gamma=0.1)\n",
    "w_ls, _ = least_squares(x_train[:,2], tx)\n",
    "\n",
    "print(\"Weights from GD: \", w_gd)\n",
    "print(\"Weights from SGD: \", w_sgd)\n",
    "print(\"Weights from LS: \", w_ls)\n",
    "\n",
    "plt.scatter(x_train[:, 1], x_train[:, 2], alpha=0.2)\n",
    "plt.plot(x_train[:, 1], tx @ w_gd, label='GD', color='orange')\n",
    "plt.plot(x_train[:, 1], tx @ w_sgd, label='SGD', color='green')\n",
    "plt.plot(x_train[:, 1], tx @ w_ls, label='LS', color='red', linestyle='dashed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d293abc",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760daf1",
   "metadata": {},
   "source": [
    "\n",
    "Dealing with NaN values, correlated columns, categoric variable.\n",
    "Handle outliers, encode categorical, feature normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_mask = ~np.isnan(x_train).any(axis = 0)\n",
    "non_nan_indices = np.where(non_nan_mask)[0]\n",
    "\n",
    "print(\"Indices of features with no NaN values:\", non_nan_indices)\n",
    "print(\"Number of features without NaNs out of 321:\", non_nan_mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118df7a",
   "metadata": {},
   "source": [
    "### Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22222c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_imp, x_test_imp = mean_imputation(x_train, x_test, train_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ab71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_nan_mask = ~np.isnan(x_train_imp).any(axis = 0)\n",
    "non_nan_indices = np.where(non_nan_mask)[0]\n",
    "\n",
    "print(\"Indices of features with no NaN values:\", non_nan_indices)\n",
    "print(\"Number of features without NaNs out of 321:\", non_nan_mask.sum())\n",
    "print(\"Number of dropped features\", 321- non_nan_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm, x_test_norm = normalize(x_train_imp, x_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make boxplot with colors\n",
    "box = plt.boxplot(x_train_norm, patch_artist=True, notch=True, widths=0.6)\n",
    "\n",
    "\n",
    "\n",
    "# Labels\n",
    "plt.title(\"Boxplot of All Features (Normalized)\", fontsize=16)\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x_train_norm[:, 0], bins=30)\n",
    "plt.title(\"Feature 0 distribution after normalization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(x_train_norm, rowvar=False)  # shape (D, D)\n",
    "\n",
    "# Step 2: Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed099086",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range (corr_matrix.shape[0]):\n",
    "    for j in range (i+1, corr_matrix.shape[0]):\n",
    "        if abs(corr_matrix[i,j]) > threshold:\n",
    "            high_corr_pairs.append((i,j))\n",
    "            \n",
    "            print(f\"Features {train_columns[i]} and {train_columns[j]} have correlation {corr_matrix[i,j]:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(corr_matrix == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_corr_pairs = []\n",
    "\n",
    "for i in range (corr_matrix.shape[0]):\n",
    "    for j in range (i+1, corr_matrix.shape[0]):\n",
    "        if abs(corr_matrix[i,j]) > 0.99:\n",
    "            one_corr_pairs.append((i,j))\n",
    "            \n",
    "            print(f\"Features {train_columns[i]} and {train_columns[j]} have correlation {corr_matrix[i,j]:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fb012",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f4f8f",
   "metadata": {},
   "source": [
    "I'm arrived till index 145 (ASTHMAGE) which needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cef1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns[293]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ca5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default missing codes for all features\n",
    "DEFAULT_MISSING = [7, 9]\n",
    "\n",
    "EXCEPTIONS = {}\n",
    "\n",
    "for i in range(0, 24):\n",
    "    EXCEPTIONS[i] = []\n",
    "    \n",
    "for i in [216,217,218, 248, 249, 250, 251, 253, 254, 266, 267, 268, 269, 270,271, 276, 277, 285, 286, 291, 292, 295, 296, 299, 300, 301, 302, 303, 304]:\n",
    "    EXCEPTIONS[i] = []\n",
    "\n",
    "for i in [252]:\n",
    "    EXCEPTIONS[i] = [99999] # don't know\n",
    "    \n",
    "for i in [262]:\n",
    "    EXCEPTIONS[i] = [900] # don't know\n",
    "\n",
    "for i in [264, 287, 288, 293, 294, 297]:\n",
    "    EXCEPTIONS[i] = [99900] # don't know \n",
    "\n",
    "for i in [242]:\n",
    "    EXCEPTIONS[i] = [9] # don't know\n",
    "    \n",
    "for i in [246]:\n",
    "    EXCEPTIONS[i] = [14] # don't know\n",
    "    \n",
    "for i in [247]:\n",
    "    EXCEPTIONS[i] = [3] # don't know\n",
    "       \n",
    "for i in [88]:\n",
    "    EXCEPTIONS[i] = [98,77, 99] # 98 other, 77 don't know, 99 refused\n",
    "    \n",
    "for i in [60, 78, 80, 98, 119, 122, 168, 224, 225,239, 240 ]:\n",
    "    EXCEPTIONS[i] = [77, 99] # 77 don't know, 99 refused\n",
    "\n",
    "for i in [27,28,29,79,112, 114, 206, 207, 208, 209, 210, 211, 212, 213]:\n",
    "    EXCEPTIONS[i] = [77, 88, 99] # 77 don't know, 88 None, 99 refused\n",
    "    \n",
    "for i in [33, 58, 99, 115, 118, 132, 151, 152, 153, 154, 192, 193]:\n",
    "    EXCEPTIONS[i] = [7, 8, 9] # 8 never and not able to work or not applicable\n",
    "    \n",
    "for i in [147,148]:\n",
    "    EXCEPTIONS[i] = [88, 98] # 88 none , 98 don't know\n",
    "    \n",
    "for i in [195, 197]:\n",
    "    EXCEPTIONS[i] = [97, 98, 99] # 97 don't know , 98 zero,  99 refused\n",
    "        \n",
    "for i in [49, 145]:\n",
    "    EXCEPTIONS[i] = [98, 99] # 98 don't know , 99 refused\n",
    "\n",
    "for i in [59]:\n",
    "    EXCEPTIONS[i] = [88, 99] # 88 None, 99 refused\n",
    "    \n",
    "for i in [62,63]:\n",
    "    EXCEPTIONS[i] = [7777, 9999] # 7777 don't know, 9999 refused\n",
    "\n",
    "for i in [75]:\n",
    "    EXCEPTIONS[i] = [8, 77, 99]  # 8 Never, 77 don't know, 99 refused\n",
    "\n",
    "for i in [77, 94, 110, 150]:\n",
    "    EXCEPTIONS[i] = [777, 888, 999] # 777 don't know, 888 no drinks/ never, 999 refused\n",
    "    \n",
    "for i in range(81, 87):\n",
    "    EXCEPTIONS[i] = [555, 777, 999] # 555 Never, 777 don't know, 999 refused\n",
    "\n",
    "for i in [75, 130]:\n",
    "    EXCEPTIONS[i] = [8, 77, 99] # 8 Never/ non applicable, 77 don't know, 99 refused\n",
    "    \n",
    "for i in [60, 78, 80, 102, 106]:\n",
    "    EXCEPTIONS[i] = [77, 99] # 77 don't know, 99 refused\n",
    "\n",
    "for i in [89,90,92, 93]:\n",
    "    EXCEPTIONS[i] = [777, 999] # 777 don't know, 999 refused\n",
    "\n",
    "for i in [91, 113]:\n",
    "    EXCEPTIONS[i] = [88, 98, 77, 99] # 88 No other activity, 98 other/never heard, 77 don't know, 99 refused\n",
    "\n",
    "for i in [101,105]:\n",
    "    EXCEPTIONS[i] = [777777, 999999] # 777777 don't know, 999999 refused]   \n",
    "    \n",
    "for i in [111, 143]:\n",
    "    EXCEPTIONS[i] = [555, 777,888, 999] # 555 No feet, 777 don't know, 888 never, 999 refused\n",
    "    \n",
    "for i in [127,128]:\n",
    "    EXCEPTIONS[i] = [7,8] # 7 don't know, 8 not applicable\n",
    "\n",
    "for i in [129, 137, 138,139, 140]:\n",
    "    EXCEPTIONS[i] = [5,7,9] # 5 never, 7 don't know, 9 refused \n",
    "\n",
    "for i in [131]:\n",
    "    EXCEPTIONS[i] = [5,7,8,9] # 5 never, 7 don't know, 8 not applicable 9 refused \n",
    "    \n",
    "for i in [148,149]:\n",
    "    EXCEPTIONS[i] = [88, 98, 99] # 88 none, 98 don't know , 99 refused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_with_exceptions(x, feature_index, EXCEPTIONS):\n",
    "    \"\"\"\n",
    "    Replace default missing codes and any feature-specific exceptions with NaN\n",
    "    \"\"\"\n",
    "    x = x.astype(float)\n",
    "    \n",
    "    # Start with default missing codes\n",
    "    codes_to_replace = list(DEFAULT_MISSING)\n",
    "    \n",
    "    # Add feature-specific codes if present\n",
    "    if feature_index in EXCEPTIONS:\n",
    "        codes_to_replace.extend(EXCEPTIONS[feature_index])\n",
    "    \n",
    "    # Replace with NaN\n",
    "    for code in codes_to_replace:\n",
    "        x[x == code] = np.nan\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ac823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(X, EXCEPTIONS):\n",
    "    \"\"\"\n",
    "    Clean the full dataset using DEFAULT_MISSING and feature-specific EXCEPTIONS\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    X_clean = np.empty_like(X, dtype=float)\n",
    "    \n",
    "    for j in range(n_features):\n",
    "        X_clean[:, j] = replace_missing_with_exceptions(X[:, j], j, EXCEPTIONS)\n",
    "    \n",
    "    return X_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec6c91",
   "metadata": {},
   "source": [
    "## Cleaning Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84e5a4",
   "metadata": {},
   "source": [
    "IDATE can be discarded (we have IDAY, IMONTH, IYEAR).\n",
    "LADULT\tNUMADULT\tNUMMEN\tNUMWOMEN are continuos (9 has a value but don't display a lot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPECIAL_CODES = [7, 9, 77, 99, 777, 999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_feature_type(x, cat_threshold=11):\n",
    "    \"\"\"\n",
    "    Automatically detect feature type.\n",
    "    - x: 1D numpy array (feature column)\n",
    "    - cat_threshold: maximum number of unique valid values to consider categorical\n",
    "    Returns: 'categorical' or 'continuous'\n",
    "    \"\"\"\n",
    "    x = x.astype(float)\n",
    "    # Step 1: separate special codes and valid values\n",
    "    valid_mask = ~np.isin(x, SPECIAL_CODES) & ~np.isnan(x)\n",
    "    valid_vals = x[valid_mask]\n",
    "    \n",
    "    # Step 2: decide based on number of unique valid values\n",
    "    if len(np.unique(valid_vals)) <= cat_threshold:\n",
    "        return \"categorical\"\n",
    "    else:\n",
    "        return \"continuous\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_codes_with_nan(x):\n",
    "    \"\"\"\n",
    "    Replace all SPECIAL_CODES in x with np.nan\n",
    "    \"\"\"\n",
    "    x = x.astype(float)  # ensure float so we can assign NaN\n",
    "    for code in SPECIAL_CODES:\n",
    "        x[x == code] = np.nan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_dictionary(X, cat_threshold=20):\n",
    "    \"\"\"\n",
    "    Build a dictionary describing each feature:\n",
    "    - type: categorical or continuous\n",
    "    - invalid: list of special codes present in the feature\n",
    "    - X_clean: data with special codes replaced by NaN\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    feature_info = {}\n",
    "    X_clean = np.empty_like(X, dtype=float)\n",
    "    \n",
    "    for j in range(n_features):\n",
    "        col = X[:, j]\n",
    "        # Step 1: replace special codes with NaN\n",
    "        col_clean = replace_special_codes_with_nan(col)\n",
    "        X_clean[:, j] = col_clean\n",
    "        \n",
    "        # Step 2: detect feature type safely\n",
    "        ftype = detect_feature_type(col_clean, cat_threshold)\n",
    "        \n",
    "        # Step 3: detect which special codes were present\n",
    "        present_codes = [code for code in SPECIAL_CODES if code in col]\n",
    "        \n",
    "        # Step 4: store info\n",
    "        feature_info[j] = {\n",
    "            \"type\": ftype,\n",
    "            \"invalid\": present_codes\n",
    "        }\n",
    "    \n",
    "    return feature_info, X_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a493e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_info, X_clean = build_feature_dictionary(df.values)\n",
    "\n",
    "print(\"Feature types & invalid codes:\")\n",
    "for k, v in feature_info.items():\n",
    "    print(k, v)\n",
    "\n",
    "print(\"\\nCleaned dataset:\")\n",
    "print(X_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73785079",
   "metadata": {},
   "source": [
    "7,77 don't know \\\n",
    "9,99 refused \\\n",
    "PHYSHLTH, POORHLTH ecc 8, 88 is None (to deal with) \\\n",
    "DIABAGE2 98 is none... \\\n",
    "EMPLOY1 8 is not don't know but is unable to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f60dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rules = {}\n",
    "\n",
    "for col in range(x_train.shape[1]):\n",
    "    col_type = detect_feature_type(x_train[:, col])\n",
    "    \n",
    "    if col_type == \"continuous\":\n",
    "        feature_rules[col] = {\"type\": \"continuous\", \"invalid\": []}\n",
    "    else:\n",
    "        feature_rules[col] = {\"type\": \"categorical\", \"invalid\": [7, 9, 77, 99]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b576f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns[145]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccf1c3",
   "metadata": {},
   "source": [
    "### Altro tentativo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default missing codes for all features\n",
    "DEFAULT_MISSING = [7, 9]\n",
    "\n",
    "EXCEPTIONS = {}\n",
    "\n",
    "for i in range(0, 24):\n",
    "    EXCEPTIONS[i] = []\n",
    "\n",
    "for i in [88]:\n",
    "    EXCEPTIONS[i] = [98,77, 99] # 98 other, 77 don't know, 99 refused\n",
    "    \n",
    "for i in [60, 78, 80, 98, 119, 122]:\n",
    "    EXCEPTIONS[i] = [77, 99] # 77 don't know, 99 refused\n",
    "\n",
    "for i in [27,28,29,79,112, 114]:\n",
    "    EXCEPTIONS[i] = [77, 88, 99] # 77 don't know, 88 None, 99 refused\n",
    "    \n",
    "for i in [33, 58, 99, 115, 118, 132]:\n",
    "    EXCEPTIONS[i] = [7, 8, 9] # 8 never and not able to work\n",
    "    \n",
    "for i in range(49, 50):\n",
    "    EXCEPTIONS[i] = [98, 99] # 98 don't know , 99 refused\n",
    "\n",
    "for i in [59]:\n",
    "    EXCEPTIONS[i] = [88, 99] # 88 None, 99 refused\n",
    "    \n",
    "for i in [62,63]:\n",
    "    EXCEPTIONS[i] = [7777, 9999] # 7777 don't know, 9999 refused\n",
    "\n",
    "for i in [75]:\n",
    "    EXCEPTIONS[i] = [8, 77, 99]  # 8 Never, 77 don't know, 99 refused\n",
    "\n",
    "for i in [77, 94, 110]:\n",
    "    EXCEPTIONS[i] = [777, 888, 999] # 777 don't know, 888 no drinks/ never, 999 refused\n",
    "    \n",
    "for i in range(81, 87):\n",
    "    EXCEPTIONS[i] = [555, 777, 999] # 555 Never, 777 don't know, 999 refused\n",
    "\n",
    "for i in [75, 130]:\n",
    "    EXCEPTIONS[i] = [8, 77, 99] # 8 Never/ non applicable, 77 don't know, 99 refused\n",
    "    \n",
    "for i in [60, 78, 80, 102, 106]:\n",
    "    EXCEPTIONS[i] = [77, 99] # 77 don't know, 99 refused\n",
    "\n",
    "for i in [89,90,92, 93]:\n",
    "    EXCEPTIONS[i] = [777, 999] # 777 don't know, 999 refused\n",
    "\n",
    "for i in [91, 113]:\n",
    "    EXCEPTIONS[i] = [88, 98, 77, 99] # 88 No other activity, 98 other/never heard, 77 don't know, 99 refused\n",
    "\n",
    "for i in [101,105]:\n",
    "    EXCEPTIONS[i] = [777777, 999999] # 777777 don't know, 999999 refused]   \n",
    "    \n",
    "for i in [111, 143]:\n",
    "    EXCEPTIONS[i] = [555, 777,888, 999] # 555 No feet, 777 don't know, 888 never, 999 refused\n",
    "    \n",
    "for i in [127,128]:\n",
    "    EXCEPTIONS[i] = [7,8] # 7 don't know, 8 not applicable\n",
    "\n",
    "for i in [129, 137, 138,139, 140]:\n",
    "    EXCEPTIONS[i] = [5,7,9] # 5 never, 7 don't know, 9 refused \n",
    "\n",
    "for i in [131]:\n",
    "    EXCEPTIONS[i] = [5,7,8,9] # 5 never, 7 don't know, 8 not applicable 9 refused \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbfc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_with_nan(x, feature_index):\n",
    "    \"\"\"\n",
    "    Replace default missing codes and any feature-specific exceptions with NaN\n",
    "    \"\"\"\n",
    "    x = x.astype(float)\n",
    "    \n",
    "    # Start with default missing codes\n",
    "    codes_to_replace = list(DEFAULT_MISSING)\n",
    "    \n",
    "    # Add feature-specific codes if present\n",
    "    if feature_index in EXCEPTIONS:\n",
    "        codes_to_replace.extend(EXCEPTIONS[feature_index])\n",
    "    \n",
    "    # Replace with NaN\n",
    "    for code in codes_to_replace:\n",
    "        x[x == code] = np.nan\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff893003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_exceptions(X):\n",
    "    n_features = X.shape[1]\n",
    "    X_clean = np.empty_like(X, dtype=float)\n",
    "    \n",
    "    for j in range(n_features):\n",
    "        X_clean[:, j] = replace_missing_with_nan(X[:, j], j)\n",
    "    \n",
    "    return X_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d1d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
