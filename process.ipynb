{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6faa0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from implementations import *\n",
    "from preprocessing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58339d",
   "metadata": {},
   "source": [
    "# Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14cdb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_folder = './data/'\n",
    "data_folder = 'C:/Users/ACER/OneDrive - epfl.ch/Desktop/ML/dataset/'\n",
    "#data_folder = \"C:/Users/plane/OneDrive/Bureau/MilaLyon/MilaLyon/data/\"\n",
    "#data_folder = \"C:/Users/ACER/OneDrive - epfl.ch/Desktop/ML/MilaLyon/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358374f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = load_csv_data(data_folder, max_rows=None, dictionnary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c61cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: <class 'numpy.ndarray'> with shape (328135, 321)\n",
      "x_test: <class 'numpy.ndarray'> with shape (109379, 321)\n",
      "y_train: <class 'numpy.ndarray'> with shape (328135,)\n",
      "train_ids: <class 'numpy.ndarray'> with shape (328135,)\n",
      "test_ids: <class 'numpy.ndarray'> with shape (109379,)\n",
      "feature_names: <class 'numpy.ndarray'> with shape (321,)\n",
      "useless: <class 'numpy.ndarray'> with shape (321,)\n",
      "health_related: <class 'numpy.ndarray'> with shape (321,)\n",
      "better_elsewhere: <class 'numpy.ndarray'> with shape (321,)\n",
      "bad_format_no_better: <class 'numpy.ndarray'> with shape (321,)\n",
      "binary: <class 'numpy.ndarray'> with shape (321,)\n",
      "one_hot: <class 'numpy.ndarray'> with shape (321,)\n",
      "zero_values: <class 'numpy.ndarray'> with shape (321,)\n",
      "default_values: <class 'numpy.ndarray'> with shape (321,)\n",
      "ordinal: <class 'numpy.ndarray'> with shape (321,)\n",
      "continuous: <class 'numpy.ndarray'> with shape (321,)\n"
     ]
    }
   ],
   "source": [
    "data_copy = dict(csv_data)\n",
    "preprocess_data(data_copy, nan_drop_threshold=0.9, correlation_threshold=0.01, n_std=3, only_health_related=False, split_val=True, val_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f502a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapes(data_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7c2f0",
   "metadata": {},
   "source": [
    "Our dataset is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the frequencies of each class\n",
    "class_counts = [len(data_copy['y_train'][data_copy['y_train'] == 0]), len(data_copy['y_train'][data_copy['y_train'] == 1])]\n",
    "\n",
    "print(f\"Ratio of majority class to minority class: {class_counts[0] / class_counts[1]:.2f}\")\n",
    "\n",
    "# Plot the class frequencies using a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Majority Class (Class 0)', 'Minority Class (Class 1)'], class_counts, color=['blue', 'red'])\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Frequencies in an Imbalanced Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a095b",
   "metadata": {},
   "source": [
    "# Model Selection and Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e030c",
   "metadata": {},
   "source": [
    "## Learning Rate Selection in Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce34708",
   "metadata": {},
   "source": [
    "Crossvalidation to find best learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7abe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the generic cross-validation function\n",
    "def learning_function(y, tx, gamma):\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    return logistic_regression(y, tx, initial_w, max_iters=1000, gamma=gamma)\n",
    "\n",
    "gammas = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "best_gamma, results = cross_validate_hyperparameter(data_copy['y_train'], data_copy['x_train'], learning_function, gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ad7ea",
   "metadata": {},
   "source": [
    "Even if the best gamma found is more than 0.5, we observe oscillations in the loss function for this value. We thus choose gamma = 0.5 as a trade-off between convergence speed and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11462ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gamma = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best gamma and get training history\n",
    "weights, losses = logistic_regression(data_copy['y_train'], data_copy['x_train'], np.zeros(data_copy['x_train'].shape[1]), max_iters=500, gamma=best_gamma, return_history=True)\n",
    "# Plot training and validation performance\n",
    "plot_training_validation_performance(data_copy['x_train'], data_copy['y_train'], data_copy['x_val'], data_copy['y_val'], weights, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743460a",
   "metadata": {},
   "source": [
    "## Lambda tuning in ridge logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the generic cross-validation function\n",
    "def learning_function(y, tx, lambda_):\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    return reg_logistic_regression(y, tx, lambda_, initial_w, max_iters=150, gamma=best_gamma)\n",
    "\n",
    "lambdas = [1e-6, 1e-4, 1e-2, 1e-1]\n",
    "\n",
    "best_lambda, results = cross_validate_hyperparameter(data_copy['y_train'], data_copy['x_train'], learning_function, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best gamma and get training history\n",
    "weights, losses = reg_logistic_regression(data_copy['y_train'], data_copy['x_train'], best_lambda, np.zeros(data_copy['x_train'].shape[1]), max_iters=150, gamma=best_gamma, return_history=True)\n",
    "# Plot training and validation performance\n",
    "plot_training_validation_performance(data_copy['x_train'], data_copy['y_train'], data_copy['x_val'], data_copy['y_val'], weights, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81410ec9",
   "metadata": {},
   "source": [
    "## Polynomial Feature Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1,2,3]\n",
    "best_degree, results = cross_validate_degrees(data_copy['x_train'], data_copy['y_train'], degrees, data_copy['continuous'], k=5, max_iters=150, gamma=best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b475c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_poly = build_poly(data_copy['x_train'], best_degree, to_expand=data_copy['continuous'])\n",
    "x_val_poly = build_poly(data_copy['x_val'], best_degree, to_expand=data_copy['continuous'])\n",
    "x_test_poly = build_poly(data_copy['x_test'], best_degree, to_expand=data_copy['continuous'])\n",
    "\n",
    "weights, losses = logistic_regression(data_copy['y_train'], x_train_poly, np.zeros(x_train_poly.shape[1]), max_iters=150, gamma=best_gamma, return_history=True)\n",
    "\n",
    "plot_training_validation_performance(x_train_poly, data_copy['y_train'], x_val_poly, data_copy['y_val'], weights, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd702347",
   "metadata": {},
   "source": [
    "## Weighted logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the generic cross-validation function\n",
    "def learning_function(y, tx, relative_weight):\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    sample_weights = 1 + (relative_weight - 1) * y\n",
    "    return weighted_reg_logistic_regression(y, tx, 0, sample_weights, initial_w, max_iters=150, gamma=best_gamma)\n",
    "\n",
    "relative_weights = [1, 4, 6, 10, 14]\n",
    "best_relative_weight, results = cross_validate_hyperparameter(data_copy['y_train'], data_copy['x_train'], learning_function, relative_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = 1 + (best_relative_weight - 1) * data_copy['y_train']\n",
    "initial_w = np.zeros(data_copy['x_train'].shape[1])\n",
    "weights, losses = weighted_reg_logistic_regression(data_copy['y_train'], data_copy['x_train'], 0, sample_weights, initial_w, 150, best_gamma, return_history=True)\n",
    "\n",
    "plot_training_validation_performance(data_copy['x_train'], data_copy['y_train'], data_copy['x_val'], data_copy['y_val'], weights, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3ecc5",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b61188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_function(y, tx, oversampling_ratio):\n",
    "    x_over, y_over = oversample_data(tx, y, ratio=oversampling_ratio)\n",
    "    initial_w = np.zeros(x_over.shape[1])\n",
    "    return logistic_regression(y_over, x_over, initial_w, max_iters=150, gamma=best_gamma)\n",
    "\n",
    "ratios = [0.2, 0.4, 0.6, 0.8]\n",
    "best_ratio, results = cross_validate_hyperparameter(data_copy['y_train'], data_copy['x_train'], learning_function, ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_over, y_over = oversample_data(data_copy['x_train'], data_copy['y_train'], ratio=best_ratio)\n",
    "initial_w = np.zeros(x_over.shape[1])\n",
    "weights, losses =logistic_regression(y_over, x_over,initial_w, 150, best_gamma, return_history=True)\n",
    "\n",
    "plot_training_validation_performance(x_over, y_over, data_copy['x_val'], data_copy['y_val'], weights, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23747d20",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140df2a6",
   "metadata": {},
   "source": [
    "Based on the best learning rate, best lambda, best weights for the minority class, and best polynomial degree, we generate the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "# For the final model, we use the maximum number of samples\n",
    "# x_train_full contains all training samples (train + val)\n",
    "x_train_full = np.concatenate([data_copy['x_train'], data_copy['x_val']])\n",
    "y_train_full = np.concatenate([data_copy['y_train'], data_copy['y_val']])\n",
    "\n",
    "# Polynomial Feature Expansion\n",
    "x_train_poly = build_poly(x_train_full, best_degree)\n",
    "x_test_poly = build_poly(data_copy['x_test'], best_degree)\n",
    "\n",
    "# Best weights for the minority class\n",
    "sample_weights = 1 + (best_relative_weight - 1) * y_train_full\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "weights, losses = weighted_reg_logistic_regression(y_train_full, x_train_poly, best_lambda, sample_weights, np.zeros(x_train_poly.shape[1]), max_iters=1000, gamma=best_gamma, return_history=True)\n",
    "\n",
    "# Plot the loss curve for the final model to check convergence\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve for Final Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Determine the best threshold on the full training set\n",
    "best_t, _ = best_threshold(y_train_full, x_train_poly, weights[-1])\n",
    "\n",
    "# Predict on test set and create submission file\n",
    "test_pred = predict_labels_logistic(x_test_poly, weights[-1], threshold=best_t)\n",
    "#Convert test_pred to 1 and -1\n",
    "test_pred = np.where(test_pred == 1, 1, -1)\n",
    "create_csv_submission(data_copy['test_ids'], test_pred, 'logistic_regression_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b533e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
